# ANT332 Use Amazon ES to visualize and monitor containerized applications

In this lab we will learn to capture logs and monitor metrics from Amazon Elastic Kubernetes Service (Amazon EKS) using a variety of collection agents and analyzing them using Amazon Elasticsearch service (Amazon ES).


![alt text](https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/Overall+architecture.png "Logo Title Text 1")

# What is Amazon Elasticsearch?
Amazon Elasticsearch Service is a fully managed service that makes it easy for you to deploy, secure, and operate Elasticsearch at scale with zero down time. The service offers open-source Elasticsearch APIs, managed Kibana, and integrations with Logstash and other AWS Services, enabling you to securely ingest data from any source and search, analyze, and visualize it in real time. Amazon Elasticsearch Service lets you pay only for what you use â€“ there are no upfront costs or usage requirements. With Amazon Elasticsearch Service, you get the ELK stack you need, without the operational overhead.

![alt text](
https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/How+AES+works_final.2e3ac88fbb9910d7c401d0748556db0c91c97b33.png)

# What is Amazon EKS?
Amazon Elastic Kubernetes Service (Amazon EKS) makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS.

Amazon EKS runs the Kubernetes management infrastructure for you across multiple AWS availability zones to eliminate a single point of failure. Amazon EKS is certified Kubernetes conformant so you can use existing tooling and plugins from partners and the Kubernetes community. Applications running on any standard Kubernetes environment are fully compatible and can be easily migrated to Amazon EKS.

Amazon EKS supports both Windows Containers and Linux Containers to enable all your use cases and workloads.


![alt text](
https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/product-page-diagram-AmazonEKS-v2.dd41321fd3aa0915b93396c13e739351d2160ba8.png)


# Kubernetes 101
@saad

# Monitoring Kubernetes

To get full visibility into your applications we need two key elements, metrics and logs for the following aspect.

#### Infrastructure level
This level will have data pertaining to the worker nodes. For example metrics such as CPU percentage, memory used and also system logs.

#### Kubernetes level
This level involves everything at the Kubernetes level including pods, containers, error logs and much more.

In this lab we will collect data from both these levels. There's multilple ways to capture and process this data. Lets take a look at two most popular way our customers love to do it.

# Data pipline using Beats and Logstsah
In this lab will discuss two different pipelines involving different technologies to collect the metrics. You can use either in production depending on your needs.

Beats is a family of popular open-source data collection agents. Logstash is a popular open-source,server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then pushes the result to various destinations. Logstash acts at the parsing and buffering layer de-coupling the source and the destination. For this lab, we will deploy multiple Logstash instances in an Auto Scaling Group that scale based on resource needs.

#### Metricbeat -> Logstash -> Amazon ES
 In this case we will use metricbeat to collect system and Kubernetes metrics and send it out to Logstash.

#### Filebeat -> Logstash -> Amazon ES
Filebeat is a light-weight agent that can tail log files and push the data to Logstash.

# Data pipline using Fluentd and Fluentbit

Similarly to the previous pipeline, Fluentd is another popular open-source alternative to collect metrics and logs. Fluentbit is part of the same family of product and acts as a more light-weight forwarder.

#### Fluentbit (metrics) -> Fluentd -> Amazon ES
In this case, we will use Fluentbit, a light-weight agent to collect metrics from Redis and write into Fluentd. Fluentd will parse the data and index into Amazon ES

#### Fluentd -> Amazon ES
Fluentd can act as both the data collector and the data parser. We will define various log inputs to Fluentd.


# Let the lab begin!
@Saad
In order to save time, we have already pre-deployed the environment for you. You account will have the following resources

* AWS Cloud9 IDE environment (this is where you'll spend the most time for this lab)
* Amazon ES domain
* Amazon EKS Cluster, API server, worker nodes
* Bastion host (Amazon EC2 instance) with SSH key

# What we will be monitoring today
We will use a simple 'Guestbook' application, which is a multi-tier web application using Kubernetes and Docker. This example consists of a guestbook application with the following components:

* A single-instance Redis master to store guestbook entries
* Multiple replicated Redis instances to serve reads
* Multiple web frontend instances



![alt text](
https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/guestbook_architecture.png)
# Getting familiar with today's lab environment
For the lab today, you will primarily use two interfaces - AWS Cloud9 and Kibana. We recommend keeping both these open in your broswer as there will significant forth.

**AWS Cloud9** is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. We will run all our commands from here.

**Kibana** is an open source data visualization plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. We will use Kibana to visualize all the logs and metrics.

# Configure Cloud9 IDE
*TBD Disable role in settings*

Run the following commands to check if EKS is configured correctly.

**Intro to kubectl**

```
kubectl get svc
```
*TBD you should see the output below*
@saad

# Configure Kibana

Amazon ES supports **public access** domains, which can receive requests from any internet-connected device, and **VPC access** domains, which are isolated from the public internet and hence highly recommended.

In this lab we have deployed Amazon ES with **VPC access** as its more secure. This endpoint is only visible to the servers inside that same VPC. To access Kibana from your laptop we have to create a SSH tunnel from your laptop to a *Bastion host* running inside the same VPC. Once you setup this up, you can access Kibana directly from your local broswer on your laptop.

To eshtablish an SSH tunnel, you need the following three key details

**Step 1: Public IP address of the Bastion host and Amazon ES endpoint URL**

You can find this under Services -> Cloudformation -> Click on the bottom most stack -> go to Outputs -> Copy IP address next to ```OutputFromNestedBastionStack``` -> Copy Amazon ES endpoint under ```OutputFromNestedNetworkStack```

![alt text](https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/bastion-step1.png)

![alt text](https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/bastion-step2.png)

![alt text](https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/bastion-step3.png)

![alt text](https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/bastion-step4.png)


**Step 2: SSH key file for Bastion host**

**Step 3: Establish SSH tunnel**

@saad windows and mac laptop
*TBD Instructions for SSH tunnel*

**Step 4: Connect to Kibana with your local browser**

Click on this deep link to open Kibana in local browser on your laptop and on the welcome screen click **Explore on my own**

http://localhost:9200/_plugin/kibana

You will will taken to the Kibana home page with many controls and dashboards. Kibana does not have any data yet, so lets switch back to Cloud9 to start emitting data to Amazon ES.

# Deploying Logstash
*TBD @saad steps to deploy logstash and config files*

Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all your data for diverse advanced downstream analytics and visualization use cases.

A Logstash pipeline has three key elements **inputs, filters and outputs**. The input plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write the data to a destination.

In this lab we will built the following pipeline to parse *TBD log* and output to Amazon ES.
@Saad

```
input logstash config details here
```

# Metricbeat 101
*TBD review dashboard, deployment steps*

Metricbeat is a lightweight open-source agent to collect system and service statistics.
Metricbeat consists of modules and metricsets. A Metricbeat module defines the basic logic for collecting data from a specific service, such as Redis, MySQL, and so on. The module specifies details about the service, including how to connect, how often to collect metrics, and which metrics to collect.

In this lab we enable **system** and **kubernetes** modules. For more information open the metricbeat.yaml file.

```
- module: system
     period: 10s
     metricsets:
       - cpu
       - load
       - memory
       ....
- module: kubernetes
      metricsets:
        - node
        - system
        - pod
        ....
```


Note: Metricbeat gets its key metrics from a internal kubernetes service called **kube-state-metrics** It is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. It is focused on the health of the various objects inside, such as deployments, nodes and pods.

#### Install kube-state-metrics
**Run** the following command to install kube-state-metricsets

```
kubectl apply -f kube-state-metrics-cluster-role-binding.yaml &&
kubectl apply -f kube-state-metrics-service-account.yaml &&
kubectl apply -f kube-state-metrics-cluster-role.yaml &&
kubectl apply -f kube-state-metrics-service.yaml &&
kubectl apply -f kube-state-metrics-deployment.yaml
```

If its succesfully created, you should see *createad* messages like the ones below
```clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
serviceaccount/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
service/kube-state-metrics created
deployment.apps/kube-state-metrics created
```
**Run** the following command to double-check if its successfully running. You should see ```kube-state-metrics``` listed in the result along with other internal deployments.

```
 kubectl get deployment -n kube-system
 ```


#### Deploy metricbeat
We need to configure metricbeat to the logstash endpoint.


# Launch Kibana
* *TBD create index pattern*

When you deploy Amazon ES domain, you have the choice to deploy it into a VPC

# Filebeat 101
* *Send data to logstahs for parsing*
* *Review logs via Kibana*

*@Saad*

Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them to either to Elasticsearch or Logstash for indexing.

In this lab we will configure Filebeat to collect *TBD log file* and send the data to logstash.

```
filebeat.inputs:
    - type: log
      symlinks: true
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            in_cluster: true
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"s
```

# Fluentd 101
*Walk through one Fluentd parsing configuration TBD*

Fluentd is a fully free and fully open-source log collector that instantly enables you to have a 'Log Everything' architecture with 600+ types of systems.Fluentd treats logs as JSON, a popular machine-readable format. It is written primarily in C with a thin-Ruby wrapper that gives users flexibility.



![alt](https://ant332.s3-us-west-2.amazonaws.com/ant332-lab-guide-artifacts/fluentd-architecture.png)

Fluentd has the following key components - Setup, Inputs, Filters, Parsers and Outputs. You can find the these configurations under ```values.yml``` file under ```fluentd-elasticsearch``` folder. We use Fluentd to collect and process multiple sources of logs such as apache logs, kubelet, kube-api-server and more.

#### Setup
The configuration files is the fundamental piece to connect all things together, as it allows to define which Inputs or listeners Fluentd will have and set up common matching rules to route the Event data to a specific Output.

#### Input
Extend Fluentd to retrieve and pull event logs from external sources. An input plugin typically creates a thread socket and a listen socket. It can also be written to periodically pull data from data sources.

In this snippet below the input plugin is configured to tail and collect Apache access logs that are in json format.
```
<source>
  @type tail
  path /var/log/containers/*frontend*.log
  pos_file /var/log/apache-access.log.pos
  format json
  tag apache.access
  ....
</source>
```

#### Parser
Fluentd has a pluggable system that enables the user to create their own parser formats. In the code snippet below, we are using the out-of-the-box ```apache2``` parser to parse the access logs. To parse custom logs, you can leverage ```regexp``` to support regex parsing.

```
  <filter apache.access>
     @type parser
     key_name log
     format apache2
     ....
   </filter>
```

#### Output
This plugin allows you write data to various output destinations.
In the code snippet below the output is configured to Elasticsearch endpoint and port. With this ```logstash_format``` set true, Fluentd uses the conventional index name format logstash-%Y.%m.%d.

Fluentd can also buffer data, in this case we are leverage disk based persistent buffering set by ```@type file``` setting.
```
<match apache.**>
      @type elasticsearch
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      logstash_prefix apache
      logstash_format true
      <buffer>
        @type file
        path /var/log/fluentd-buffers/apache.system.buffer
        flush_mode interval
        ....
      </buffer>
    </match>
```
# Deploying Fluentd

Lets configure Fluentd to output to your Amazon ES domain. Copy your Amazon ES endpoint from the 'Configure Kibana' step. **Run** the following command by modifying the **url** with your Amazon ES endpoint.

```
export URL2=your-amazon-es-endpoint-url
```

After this, **run** this command to deploy Fluentd.
```
helm upgrade -i --namespace=logging --set elasticsearch.host=$URL2 fluentd-elasticsearch fluentd-elasticsearch
```
# Deploy Fluentbit
*TBD walk-throgh fluentbit deployment + sidecar container*

Fluent Bit is an open source and multi-platform Log Processor and Forwarder which allows you to collect data/logs from different sources, unify and send them to multiple destinations. It's fully compatible with Docker and Kubernetes environments.

Both Fluentd and Fluent Bit share a lot of similarities, Fluent Bit is fully based on the design and experience of Fluentd architecture and general design. Choosing which one to use depends on the final needs, from an architecture perspective we can consider:
* Fluentd is a log collector, processor, and aggregator.
* Fluent Bit is a log collector and processor (it doesn't have strong aggregation features like Fluentd).

Consider Fluentd mainly as an Aggregator and Fluent Bit as a Log Forwarder, we can see both projects complement each other providing a full reliable solution.

Lets use Fluentbit to capture **metrics** from our Redis deployments. All the agents we discussed so far in this guide were be deployed as a daemonset (a POD that runs on every node of the cluster). With Fluentbit, we're deploying it as a side-car container (for redis contianers) instead. With this approach, whenever redis contaner is deployed there is a dedicated Fluentbit side-car container attached to collect all the relevant data.

Lets take a look at the ```redis-slave.yml``` deployment file for Guestbook application. This ensures that  a Fluentbit container is spun up every time a redis container launches.
```
spec:
      containers:
      - name: slave
        image: gcr.io/google_samples/gb-redisslave:v1
        resources:
        ....
        ports:
        - containerPort: 6379

      - name: redis-statistic
        image: fluent/fluent-bit:0.14.6
        resources:
        ....
```
Once its launched, Fluentbit is configured to run a custom script (loaded into the Fluentbit container path /bin/redis) to capture metrics from redis. It then forwards the data to Fluentd using the forwad protocol that was pre-setup. You can find these configurations are in ```fluent-bit.conf```.

```
<source>
  @type exec
  tag   redis
  command bash /bin/redis
  type json
  run_interval 1m
</source>

<match **>
  @type stdout
</match>

#!/bin/bash

for i in $( (printf 'info\r\n';) | nc -w 1 redis 6379 | grep ':'); do
 key=$(echo "$i" | cut -d ":" -f1 )
 value=$(echo "$i" | cut -d ":" -f2 | sed 's/\r$//g' )
 echo "{\"$key\": \"$value\"}"
done
```




# Deploy Guestbook application

# Explore in Kibana

# Mystery hunt excercies
